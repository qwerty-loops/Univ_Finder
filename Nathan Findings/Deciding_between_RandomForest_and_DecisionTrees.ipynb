{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a50f9873",
   "metadata": {},
   "source": [
    "# Based on our previous analysis, we found that Supervised Learning made more sense as opposed to Unsupervised Learning. Additionally, we narrowed down to 2 models that we wanted to further explore. 1) Decision Trees and Random Forest Trees. We tried several models under Supervised Learning to reach these 2 final models. This file explores which we want to choose in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3103f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we load the file and we begin manipulating it\n",
    "import pandas as pd\n",
    "data_path = '/Users/nathanyap/Desktop/DataMining_Project/project/Nathan Findings/TOEFL_IELTS_Combined.csv'\n",
    "df_admitsFYI = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9bb7f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 79.03%\n",
      "Decision Tree Precision: 86.37%\n",
      "Decision Tree Recall: 81.10%\n",
      "\n",
      "Random Forest Accuracy: 80.07%\n",
      "Random Forest Precision: 83.89%\n",
      "Random Forest Recall: 86.48%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# We only want to work with 5 inputs to test which model would be better\n",
    "X = df_admitsFYI[['GPA', 'GRE Total', 'TOEFL/IELTS', 'Work Exp', 'Papers']]\n",
    "y = df_admitsFYI['Status']\n",
    "\n",
    "# splitting the data to train, we will train 70% of the set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Using the decision tree\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# doing the fit\n",
    "dt_model.fit(X_train, y_train)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# making the predicition\n",
    "dt_predictions = dt_model.predict(X_test)\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# printing out the results\n",
    "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
    "dt_precision = precision_score(y_test, dt_predictions)\n",
    "dt_recall = recall_score(y_test, dt_predictions)\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "rf_precision = precision_score(y_test, rf_predictions)\n",
    "rf_recall = recall_score(y_test, rf_predictions)\n",
    "\n",
    "\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy:.2%}\")\n",
    "print(f\"Decision Tree Precision: {dt_precision:.2%}\")\n",
    "print(f\"Decision Tree Recall: {dt_recall:.2%}\")\n",
    "print()\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy:.2%}\")\n",
    "print(f\"Random Forest Precision: {rf_precision:.2%}\")\n",
    "print(f\"Random Forest Recall: {rf_recall:.2%}\")\n",
    "\n",
    "\n",
    "\n",
    "# (dt_accuracy, dt_precision, dt_recall), (rf_accuracy, rf_precision, rf_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593ce186",
   "metadata": {},
   "source": [
    "## Before we dive into the given results, it is worth noting that we prioritize the precision factor above the others. The reason for this is that in this context, of all the students that were predicted on whether they got in, how many were actually admitted? So a high precision would mean a low false positive. The second factor would be the recall factor. Which tells us, of all the students admitted, how many did we predict correctly? Even though the decision tree factor is lower, however, we feel we should still go with it because of the higher precision. In our context, we want to minimize false positives because we do not want to create an illusion where students get too hopeful when using our model. We need students to realize that there are so many factors that our model cannot capture. For example, the rating of a letter of recommendation and how relevant work experience is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61e2d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
